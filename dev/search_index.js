var documenterSearchIndex = {"docs":
[{"location":"learn/#Learning-BPE","page":"Learning BPE","title":"Learning BPE","text":"","category":"section"},{"location":"learn/","page":"Learning BPE","title":"Learning BPE","text":"We also provide the functionality to learn new BPE map accordingly.","category":"page"},{"location":"learn/","page":"Learning BPE","title":"Learning BPE","text":"# create a bpe we want to learn\njulia> mybpe = GenericBPE{String}(; oldendsym = \"@w@\", sepsym = \"=\",\n\tinput_transform = BytePairEncoding.gpt2_tokenizer, codemap = BytePairEncoding.default_codemap(),\n\tnormalizer = BytePairEncoding.UtfNormalizer(:NFKC_CF), glossaries = BytePairEncoding.Glossary([r\"[0-9]\"]))\nGenericBPE{String}(n_merge=0, endsym=@w@, sepsym==, oldendsym=@w@, input_transform=gpt2_tokenizer, glossaries=BytePairEncoding.Glossary(Regex[r\"[0-9]\"]), codemap=BytePairEncoding.CodeMap{UInt8, UInt16}(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:' ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ā':1:'Ġ', 'ġ':1:'ł', 'Ń':1:'Ń']), normalizer=UtfNormalizer(1038))\n\n# create the learner, here we set the maximum merge rank = 5000, and required minimum word frequency = 10\njulia> bper = BPELearner(mybpe, 5000, 10)\nBPELearner(bpe = GenericBPE{String}(n_merge=0, endsym=@w@, sepsym==, oldendsym=@w@, input_transform=gpt2_tokenizer, glossaries=BytePairEncoding.Glossary(Regex[r\"[0-9]\"]), codemap=BytePairEncoding.CodeMap{UInt8, UInt16}(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:' ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ā':1:'Ġ', 'ġ':1:'ł', 'Ń':1:'Ń']), normalizer=UtfNormalizer(1038)), merge = 5000, min_freq = 10)\n\n# add corpus to the learner\njulia> add!(bper, \"./test/data/corpus.en\")\nDict{String, Int64} with 4244 entries:\n  \"Ġexceedingly\" => 2\n  \"Ġqueue\"       => 3\n  \"Ġfell\"        => 2\n  \"Ġlocal\"       => 3\n  \"Ġaspiration\"  => 1\n  \"Ġadvancement\" => 1\n  \"Ġentrusts\"    => 1\n  \"Ġhad\"         => 31\n  \"Ġretained\"    => 1\n  \"Ġcowdery\"     => 1\n  \"Ġnicer\"       => 1\n  \"Ġmission\"     => 1\n  \"Ġtime\"        => 39\n  \"Ġexecuted\"    => 1\n  \"rather\"       => 1\n  \"Ġsafely\"      => 3\n  \"Ġmakes\"       => 1\n  \"Ġreach\"       => 1\n  \"these\"        => 2\n  \"Ġenough\"      => 2\n  \"Ġspot\"        => 3\n  \"Ġusers\"       => 22\n  \"Ġpasta\"       => 1\n  \"Ġmode\"        => 1\n  \"Ġlargely\"     => 2\n  \"Ġmain\"        => 7\n  ⋮              => ⋮\n\n\n# learn the bpe\njulia> learn!(bper)\nGenericBPE{String}(n_merge=1168, endsym=@w@, sepsym==, oldendsym=@w@, input_transform=gpt2_tokenizer, glossaries=BytePairEncoding.Glossary(Regex[r\"[0-9]\"]), codemap=BytePairEncoding.CodeMap{UInt8, UInt16}(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:' ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ā':1:'Ġ', 'ġ':1:'ł', 'Ń':1:'Ń']), normalizer=UtfNormalizer(1038))\n\n# test our learned bpe\njulia> mybpe(\" Is this a 😺\")\n8-element Vector{String}:\n \"Ġis@w@\"\n \"Ġthis@w@\"\n \"Ġa@w@\"\n \"Ġ=\"\n \"ð=\"\n \"Ł=\"\n \"ĺ=\"\n \"º@w@\"\n\n# dump the learned bpe\njulia> emit(bper, \"./bpe.out\")\n\"./bpe.out\"\n\nshell> head -n10 bpe.out\n:#endsym:@w@\nĠ t\nĠt h\nĠ a\nĠ i\nĠ o\nĠ s\nĠ ,@w@\nĠth e@w@\ni n\n","category":"page"},{"location":"learn/","page":"Learning BPE","title":"Learning BPE","text":"You can also use vocab = BytePairEncoding.get_vocab(mybpe, file) to get the vocabularies parallelly from given files.  And then use add!(bper, vocab) to merge the vocabularies and learn!(bper) the BPE.","category":"page"},{"location":"encode/#Encode","page":"Encode","title":"Encode","text":"","category":"section"},{"location":"encode/","page":"Encode","title":"Encode","text":"The overall encoding procedure:","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"normalize the input text.\nseparate word in glossary.\ntransform input (like tokenization) but no on glossary words.\ncodemapping for all words.\nbpe on non-glossary words.\noutput transform (like vocabulary) on all words.","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"see Utilities for some helper type/function.","category":"page"},{"location":"encode/#Basic-Type","page":"Encode","title":"Basic Type","text":"","category":"section"},{"location":"encode/","page":"Encode","title":"Encode","text":"GenericBPE{T}(;sepsym=nothing, oldendsym = nothing, endsym = oldendsym,\n\tinput_transform = nothing, output_transform = nothing,\n\tmerging_rank = Dict{Tuple{T, T}, Int}(), cache = Dict{T, Vector{T}}(),\n\tglossaries = nothing, codemap = nothing,\n\tnormalizer = nothing) where T","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"This is the signature of GenericBPE{String}. where:","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"sepsym is the extra tag for non-end word. For example: If we split \"word\" into \"wo\" and \"rd\" and we set the sepsym = \"_\", then the result would be [\"wo_\", \"rd\"].\nendsym is the extra tag for end word. This is usually equal to the value of oldendsym. For example: If we split \"word\" into \"wo\" and \"rd\" and we set the endsym = \"/\", then the result would be [\"wo\", \"rd/\"].\noldendsym is the endsym read from a bpe file.  For example: subword-mnt use \"<w/>\" as the end tag and thus need to be set accordingly. We can also set endsym to different value to override the setting.\ninput_transform: the tokenizer.\noutput_transform: some postprocessing function (like the vocabulary).\nmerging_rank: the bpe data.\ncache: cache for bpe result.\nglossaries: see Glossary\ncodemap: see Codepoint Mapping/UnMapping\nnormalizer: see Unicode Normalization","category":"page"},{"location":"#BytePairEncoding.jl","page":"Home","title":"BytePairEncoding.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pure Julia implementation of the Byte Pair Encoding(BPE) method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The design is inspired by the original python package subword-nmt and the byte-level bpe use in openai-gpt2. BytePairEncoding.jl support different tokenize method(with the help of WordTokenizers.jl). You can simply use set the tokenizer and then Learn the BPE map with it.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add BytePairEncoding","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using BytePairEncoding, WordTokenizers\n\n# using the bpe from openai gpt\njulia> bpe = Bpe(Base.download(\"https://huggingface.co/openai-gpt/resolve/main/merges.txt\"))\nGenericBPE{String}(n_merge=40000, endsym=</w>, oldendsym=</w>, input_transform=tokenize)\n\n# reset the tokenize method to do lowercase before tokenization\njulia> bpe = GenericBPE(bpe; input_transform = tokenize∘lowercase)\nGenericBPE{String}(n_merge=40000, endsym=</w>, oldendsym=</w>, input_transform=ComposedFunction{typeof(tokenize), typeof(lowercase)}(WordTokenizers.tokenize, lowercase))\n\n# segment the sentence\njulia> bpe(\"Peter Piper picked a peck of pickled peppers\")\n8-element Vector{String}:\n \"peter</w>\"\n \"piper</w>\"\n \"picked</w>\"\n \"a</w>\"\n \"peck</w>\"\n \"of</w>\"\n \"pickled</w>\"\n \"peppers</w>\"\n\n# using the byte level bpe from openai gpt2\njulia> bbpe = ByteLevelBPE(Base.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\"))\nGenericBPE{String}(n_merge=50000, input_transform=gpt2_tokenizer, codemap=BytePairEncoding.CodeMap(StepRange{Char,\nInt64}['\\0':1:' ', '\\x7f':1:' ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ā':1:'Ġ', 'ġ':1:'ł', 'Ń':1:'Ń']))\n\n# segment the sentence\njulia> bbpe(\"This is a 😺\")\n5-element Vector{String}:\n \"This\"\n \"Ġis\"\n \"Ġa\"\n \"ĠðŁĺ\"\n \"º\"\n\n# to see the origin input, set the output_transform method that unmap the codepoint\njulia> decoded_bbpe = GenericBPE(bbpe; output_transform = BytePairEncoding.UnMap(bbpe.codemap))\nGenericBPE{String}(n_merge=50000, input_transform=gpt2_tokenizer, output_transform=BytePairEncoding.UnMap(BytePairEncoding.CodeMap(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:' ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ā':1:'Ġ', 'ġ':1:'ł', 'Ń':1:'Ń'])), codemap=BytePairEncoding.CodeMap(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:' ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ā':1:'Ġ', 'ġ':1:'ł', 'Ń':1:'Ń']))\n\njulia> decoded_bbpe(\"This is a 😺\")\n5-element Vector{String}:\n \"This\"\n \" is\"\n \" a\"\n \" \\xf0\\x9f\\x98\"\n \"\\xba\"\n\njulia> join(ans)\n\"This is a 😺\"\n","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"encode.md\",\n  \"learn.md\",\n  \"utils.md\"\n]","category":"page"},{"location":"utils/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"Here are docs about some functionality we migth during the encoding process.","category":"page"},{"location":"utils/#Unicode-Normalization","page":"Utilities","title":"Unicode Normalization","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"We provide BytePairEncoding.UtfNormalizer for unicode normalization. It simply wrapBase.Unicode.normalize as a callable object that perform the normalization on input text.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"julia> norm = BytePairEncoding.UtfNormalizer(:NFKC_CF)\nUtfNormalizer(1038)\n\njulia> norm(\"i'M nOt LOWeRCaSE\")\n\"i'm not lowercase\"\n","category":"page"},{"location":"utils/#Codepoint-Mapping/UnMapping","page":"Utilities","title":"Codepoint Mapping/UnMapping","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"In the byte level bpe, we split the unicode string into UTF-8 bytes and map them into some fixed code range. We can do this with BytePairEncoding.CodeMap.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"For example, we want to map Char(0):Char(15) to 'a':'p' and Char(16) also to 'p'.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"julia> cm = BytePairEncoding.CodeMap([((Char(0):Char(15)) , ('a':'p')), Char(16)=>'p'])\nBytePairEncoding.CodeMap{UInt8, UInt8}(StepRange{Char, Int64}['\\0':1:'\\x0f', '\\x10':1:'\\x10'], StepRange{Char, Int64}['a':1:'p', 'p':1:'p'])\n\n# mapping the codepoint. \n# this is equivalent to `cm(\"\\x00\\x01\\x0f\\x10\")`\njulia> BytePairEncoding.encode(cm, \"\\x00\\x01\\x0f\\x10\")\n\"abpp\"\n\n# unmap the codedpoint.\n# this is equvalent to `BytePairEncoding.UnMap(cm)(ans)`\njulia> BytePairEncoding.decode(cm, ans)\n\"\\0\\x01\\x0f\\x0f\"\n","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"note: Note\nYou might find that the decoded string and the origin input are different, because the code mapping is not bijective.  Therefore a good design of codemap is important, or you can just use BytePairEncoding.default_codemap() for the codemap used by openai gpt2.","category":"page"},{"location":"utils/#Glossary","page":"Utilities","title":"Glossary","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"Glossary is a list of Regex that specifing some text shouldn't be split by tokenizer or BPE.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"julia> gloss = BytePairEncoding.Glossary([r\"[0-9]\", \"New York\"])\nBytePairEncoding.Glossary(Regex[r\"[0-9]\", r\"New\\ York\"])\n\njulia> gloss(\"New York123\")\n4-element Vector{String}:\n \"New York\"\n \"1\"\n \"2\"\n \"3\"\n","category":"page"}]
}
